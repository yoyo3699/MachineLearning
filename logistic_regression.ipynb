{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161b01a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9932 accuracy per iteration\n",
      "0.9987 accuracy per iteration\n",
      "0.9997 accuracy per iteration\n",
      "0.999 accuracy per iteration\n",
      "0.9994 accuracy per iteration\n",
      "0.9996 accuracy per iteration\n",
      "0.9997 accuracy per iteration\n",
      "0.9995 accuracy per iteration\n",
      "1.0 accuracy per iteration\n",
      "0.9995999599959996 accuracy per iteration\n",
      "Accuracy: 0.5618\n",
      "[SPLIT 1]\n",
      "Accuracy: 0.9988\n",
      "[SPLIT 2]\n",
      "Accuracy: 0.9988\n",
      "[SPLIT 3]\n",
      "Accuracy: 0.9988\n",
      "[SPLIT 4]\n",
      "Accuracy: 0.9988\n",
      "[SPLIT 5]\n",
      "Accuracy: 0.9988\n",
      "[SPLIT 6]\n",
      "Accuracy: 0.9988\n",
      "[SPLIT 7]\n",
      "Accuracy: 0.9988\n",
      "[SPLIT 8]\n",
      "Accuracy: 0.9988\n",
      "[SPLIT 9]\n",
      "[[19964    12]\n",
      " [   22     2]]\n",
      "0.9983\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "data = pd.read_csv(\"datafinal.csv\")#extract data\n",
    "#transform the categorical into numeric\n",
    "#the data is bank interaction types as cash out chash in ..etc.\n",
    "#X takes all the features except one feature only the \"is Fraud \" one.\n",
    "#Y is the remaining feature \"Is fraud\" and is considered the output of the classification\n",
    "\n",
    "data[\"type\"] = data[\"type\"].map({\"CASH_OUT\": 1, \"PAYMENT\": 2,\"CASH_IN\": 3, \"TRANSFER\": 4,\"DEBIT\": 5})\n",
    "#since we deal with integeres only, the \"is fraud\" feature is mapped to integer \n",
    "#0 represents \"no fraud \" and 1 represents \"fraud\"\n",
    "data[\"isFraud\"] = data[\"isFraud\"].map({0: \"No Fraud\", 1: \"Fraud\"})#string to integers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "data.drop(['nameOrig','nameDest'],axis=1,inplace=True)\n",
    "X=data.drop('isFraud',axis=1)#X takes the values of all the features except one feacture\" is fraud\"\n",
    "y=data['isFraud']#Y is the remaining feature \"Is fraud\" and is considered the output of the classification\n",
    "for i in range(1,7):#normalization of the data , data-minOf Data/ max of data= normalised data\n",
    "    max_X=max(X.iloc[:,i])\n",
    "    min_X=min(X.iloc[:,i])\n",
    "    X.iloc[:,i]=(X.iloc[:,i]-min_X)/(max_X-min_X)\n",
    "#splitting the data into 3 parts, 60% training , 20% cross validation,20% test, by using split function and entering the % we want\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)#splits data 60% train, 40% rest\n",
    "X_cv, X_test_new=np.split(X_test, 2)#40% rest is divided by 50%= 20% cv,20%test\n",
    "Y_cv , Y_test_new=np.split(y_test, 2)\n",
    "\n",
    "y_train_new=np.zeros(len(y_train))\n",
    "for i in range (len(y_train)):#mapping values of y_train from \" is fraud\" to integer values to be able to use later\n",
    "    if y_train.iloc[i] == 'Fraud':\n",
    "        y_train_new[i] =1  \n",
    "y_train_new=pd.DataFrame(y_train_new)\n",
    "Y_cv_new=np.zeros(len(Y_cv))\n",
    "for i in range (len(Y_cv)):#mapping values of y_cv from \" is fraud\" to integer values to be able to use later\n",
    "    if Y_cv.iloc[i] == 'Fraud':\n",
    "        Y_cv_new[i] =1  \n",
    "Y_test_new_1=np.zeros(len(Y_cv))\n",
    "for i in range (len(Y_test_new)):#mapping values of y_test from \" is fraud\" to integer values to be able to use later\n",
    "    if Y_test_new.iloc[i] == 'Fraud':\n",
    "        Y_test_new_1[i] =1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# the generateXvector adds 1 coulumn at the begining full of ones as Hypothesis=theta0*1+X*restThetas\n",
    "def generateXvector(X):#add coloumn of 1 in X features\n",
    "    \"\"\" Taking the original independent variables matrix and add a row of 1 which corresponds to x_0\n",
    "        Parameters:\n",
    "          X:  independent variables matrix\n",
    "        Return value: the matrix that contains all the values in the dataset, not include the outcomes variables.  \"\"\"\n",
    "    vectorX = np.c_[np.ones((len(X), 1)), X]\n",
    "    return vectorX\n",
    "#the theta_init generates array of random thetas of length features+1(+1 due to theta0), used to generate the first hypothesis\n",
    "#..then update it later using the logistic regression\n",
    "def theta_init(X):\n",
    "    \"\"\" Generate an initial value of vector θ from the original independent variables matrix\n",
    "         Parameters:\n",
    "          X:  independent variables matrix\n",
    "        Return value: a vector of theta filled with initial guess\n",
    "    \"\"\"\n",
    "    theta = np.random.randn(X+1, 1)\n",
    "    return theta\n",
    "# the sigmoid_function used to take the hypothesis value and generate value between 0 and 1 , as the y \" is fraud\" has two values\n",
    "#.. only the 0 and 1 , this function takes the hypothesis as an input hyp=np.dot(theta,X)\n",
    "def sigmoid_function(X):\n",
    "    \"\"\" Calculate the sigmoid value of the inputs\n",
    "         Parameters:\n",
    "          X:  values\n",
    "        Return value: the sigmoid value\n",
    "    \"\"\"\n",
    "    return 1/(1+math.e**(-X))\n",
    "# the Logistics_Regression used to generate the best thetas for all orders of hyp. that gives the least cost \n",
    "def Logistics_Regression(X,y,learningrate, iterations):\n",
    "        \n",
    "    y_new = y\n",
    "    cost_lst = []\n",
    "    vectorX = generateXvector(X)#add 1 coloumn full of 1's\n",
    "    theta = theta_init(len(X.iloc[0,:]))#generate random thetas of size=features+1\n",
    "    m = len(X)\n",
    "    for i in range(iterations):#loop 100 times for more accuracy, each time we calc. hypothesis with thetas and calc. cost, then take the thetas with least cost\n",
    "        gradients = 2/m * vectorX.T.dot(sigmoid_function(vectorX.dot(theta)) - y_new)#calc. gradient used in updating thetas values\n",
    "        theta = theta - learningrate * gradients#update the thetas values\n",
    "        y_pred = sigmoid_function(vectorX.dot(theta))#calculate the hypothesis with updated thetas\n",
    "        cost_value = - np.sum(np.dot(y_new.T,np.log(y_pred)+ np.dot((1-y_new).T,np.log(1-y_pred)))) /(len(y_pred))#calc cost between y_pred and y we already know\n",
    " #Calculate the loss for each training instance\n",
    "        cost_lst.append(cost_value)#append the cost in array for plotting later\n",
    "        \n",
    "    # plt.plot(np.arange(1,iterations),cost_lst[1:], color = 'red')\n",
    "    # plt.title('Cost function Graph')\n",
    "    # plt.xlabel('Number of iterations')\n",
    "    # plt.ylabel('Cost')\n",
    "    return theta, cost_value#return best theta with lower cost\n",
    "\n",
    "theta1, cost_value1=Logistics_Regression(X_train,y_train_new,0.03, 100)\n",
    "#the root_regularization takes the y_pred and y we know and calculates the cost but with different rule has a the term lamda\n",
    "#so this function loops on different values of lamda and returns the lamda that gives least cost\n",
    "def root_regularization(y_pred,y_test,theta): \n",
    "    cost_reg=100\n",
    "    best_lamda=0\n",
    "    lamda=np.array([0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24])#random generated lamda values\n",
    "    cost_lst = []\n",
    "    m = len(X)#size of X\n",
    "    N = len(y_test)#number of users\n",
    "    for l in range(np.size(lamda)):#this loops calc. cost with diff lamdas and takes the min cost and best lamda\n",
    "        term=(lamda[l]/2*N)*(np.sum(theta**2))\n",
    "        cost_value = (- np.sum(np.dot(y_test.T,np.log(y_pred)+ np.dot((1-y_test).T,np.log(1-y_pred)))) /(len(y_pred)))+term\n",
    "        cost_lst.append(cost_value)\n",
    "        if cost_value<cost_reg:#compares the min value with the new cost , if new cost is smaller it becomes the min value\n",
    "            cost_reg=cost_value\n",
    "            best_lamda=lamda[l]\n",
    "    return best_lamda,cost_reg\n",
    "\n",
    "def rmse(y_pred,y_test):#calculated cost without lamda term, another way of calc. cost\n",
    "    cost_value = (- np.sum(np.dot(y_test.T,np.log(y_pred)+ np.dot((1-y_test).T,np.log(1-y_pred)))) /(len(y_pred)))\n",
    "    return cost_value\n",
    "\n",
    "cost_cv=100\n",
    "theta1_cv=[]\n",
    "for j in range(9):#cross validation(get the best order of hypothesis with least costs)\n",
    "    vectorX=generateXvector(X_cv.iloc[:,0:j])#add 1 coloumn with 1's\n",
    "    #call logistic reg. with X train to train the model and get each suitable theta for each hypothesis order\n",
    "    theta2, cost_value1=Logistics_Regression(X_train.iloc[:,0:j],y_train_new,0.03, 100)\n",
    "   #calculate the hypothesis with the resulted best thetas from logistic regression \n",
    "    y_pred = sigmoid_function(vectorX.dot(theta2))#calculate hypothesis using best thetas\n",
    "    best_lamda,cost_reg=root_regularization(y_pred,Y_cv_new,theta2)#calculate cost with lamda\n",
    "    if cost_value1<cost_cv:# take the order of hypothesis with least cost\n",
    "        cost_cv=cost_value1\n",
    "        theta1_cv=theta2# thetas length indicated the order of the hypothesis=features+1\n",
    "    if cost_reg<cost_cv:\n",
    "        cost_cv=cost_reg\n",
    "        theta1_cv=theta2\n",
    "#test the model using the Y_test 20% and calc. the (cost) of model\n",
    "y_pred_test=sigmoid_function(np.dot(X_test_new.iloc[:,0:np.size(theta1_cv)], theta1_cv))\n",
    "cost_Y_test=rmse(y_pred_test,Y_test_new_1)\n",
    "cost_test_reg=rmse(y_pred_test,Y_test_new_1)+(best_lamda/2*len(Y_cv))*(np.sum(theta1_cv**2))\n",
    "##K fold, a method used to train the model on differenr datas to have more accurate model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "skfolds = KFold(n_splits=10) #divides the data into 10 partitions,X takes 9 and y takes the left partition\n",
    "splits = skfolds.split(X, y)\n",
    "for i, (train_index, test_index) in enumerate(splits): #split and shuffle the data and calc. accuracy each time  \n",
    "  x_train = X.iloc[train_index]\n",
    "  y_train = y.iloc[train_index]\n",
    "  x_test  = X.iloc[test_index]\n",
    "  y_test  = y.iloc[test_index]\n",
    "  clf = LogisticRegression()\n",
    "  clf.fit(x_train, y_train)\n",
    "  y_pred = clf.predict(x_test)\n",
    "  accuracy = np.mean(y_pred == y_test)\n",
    "  print(accuracy,'accuracy per iteration')\n",
    "##stratifiedKfold, a method used to train the model on differenr datas to have more accurate model\n",
    "#but it makes sure that all partitions must contain all classes types\" fraud\",\"no fraud\"\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skfolds_1 = StratifiedKFold(n_splits=9)#K=9 as it makes sure that all partitions must contain all classes types\" fraud\",\"no fraud\" and we only have 9 features.\n",
    "splits_1 = skfolds_1.split(X, y)\n",
    "for i, (train_index, test_index) in enumerate(splits_1): #split and shuffle the data \n",
    "  x_train = X.iloc[train_index]\n",
    "  y_train = y.iloc[train_index]\n",
    "  x_test  = X.iloc[test_index]\n",
    "  y_test  = y.iloc[test_index]\n",
    "  clf_1 = LogisticRegression()\n",
    "  clf_1.fit(x_train, y_train)\n",
    "  y_pred_1 = clf_1.predict(x_test)\n",
    "  accuracy = np.mean(y_pred_1 == y_test)\n",
    "  print(\"Accuracy: %.4f\"%accuracy)\n",
    "  print(\"[SPLIT %d]\"%(i+1))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics\n",
    "print(sklearn.metrics.confusion_matrix(Y_test_new_1,np.round((y_pred_test))))\n",
    "print(accuracy_score(Y_test_new_1, np.round((y_pred_test))) )#calculate the accyracy of our logistic reg. model\n",
    "##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test_new_1,  y_pred_test)#represents the relation between TruePositives and FalsePositives\n",
    "auc = metrics.roc_auc_score(Y_test_new_1, y_pred_test)\n",
    "\n",
    "#create ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff416f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
